{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pmora\\AppData\\Local\\Temp\\ipykernel_7380\\3647077253.py:39: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  result_dataFrame = pd.read_sql(query, mydb)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import calendar\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import scipy as sc\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from skimage.measure import block_reduce\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import io\n",
    "import requests\n",
    "from metpy.calc import specific_humidity_from_dewpoint\n",
    "from metpy.units import units\n",
    "import json\n",
    "import mysql.connector\n",
    "from datetime import datetime, timedelta\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "mydb = mysql.connector.connect(\n",
    "host=\"localhost\",\n",
    "user=\"root\",\n",
    "password=\"pmcm4\",\n",
    "database= \"rivercast_model\"\n",
    ")\n",
    "\n",
    "class bi_initiate_model():\n",
    "    def __init__(self):\n",
    "        self.initialize_model()\n",
    "\n",
    "    def initialize_model(self):\n",
    "        #IMPORTING\n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')  # configure GPU    utilization\n",
    "\n",
    "        mydb._open_connection()\n",
    "        query = \"SELECT * FROM rivercast_model.modelData;\"\n",
    "        result_dataFrame = pd.read_sql(query, mydb)\n",
    "        \n",
    "\n",
    "        # Specify the column to exclude (change 'column_to_exclude' to the actual column name)\n",
    "        column_to_exclude = ['Date_Time', 'RF-Intensity.1']\n",
    "\n",
    "        # Exclude the specified column\n",
    "        df = result_dataFrame.drop(column_to_exclude, axis=1, errors='ignore')\n",
    "        self.rawData = df\n",
    "        \n",
    "\n",
    "        # Now 'df' can be used as 'mainDataToDB' or for further processing\n",
    "\n",
    "        # convert month name to integer\n",
    "\n",
    "        # create datetime column\n",
    "        df[['Year', 'Month', 'Day', 'Hour']] = df[['Year', 'Month', 'Day', 'Hour']].astype(int)\n",
    "        df['Hour'] = df['Hour'].apply(lambda x: x if x < 24 else 0)\n",
    "\n",
    "        # convert year, month, day, and hour columns into timestamp\n",
    "        df['Datetime'] = df[['Year', 'Month', 'Day', 'Hour']].apply(lambda row: datetime(row['Year'], row['Month'], row['Day'], row['Hour']).isoformat(), axis=1)\n",
    "        df[\"Datetime\"] = pd.to_datetime(df[\"Datetime\"], format='ISO8601')\n",
    "\n",
    "        # assign timestamps as the data frame index\n",
    "        df.index = df[\"Datetime\"]\n",
    "        df = df.drop(['Datetime'], axis=1)\n",
    "\n",
    "        # select the parameters\n",
    "        df = df[['Waterlevel', 'Waterlevel.1', 'Waterlevel.2', 'Waterlevel.3', 'RF-Intensity', 'RF-Intensity.2', 'RF-Intensity.3', 'Precipitation', 'Precipitation.1', 'Precipitation.2', 'Humidity', 'Humidity.1', 'Humidity.2', 'Temperature', 'Temperature.1', 'Temperature.2']] \n",
    "        df = df.astype(np.float64)  # convert parameters into a double precision floating number\n",
    "        \n",
    "        # fill in missing values using linear interpolation\n",
    "        df = df.interpolate(method='linear', limit_direction='forward')\n",
    "        df = df.resample('6H').max()  # resample dataset using the max value for each 24-hours\n",
    "        df = df.rolling(120).mean().dropna()  # perform moving average smoothing\n",
    "        \n",
    "        self.sampling = df\n",
    "\n",
    "        self.rawData = df\n",
    "        \n",
    "\n",
    "        self.dataset_min = df.min()\n",
    "        self.dataset_max = df.max()\n",
    "\n",
    "        self.normalized_df = (df - self.dataset_min) / (self.dataset_max - self.dataset_min)\n",
    "\n",
    "\n",
    "        self.cleanData = self.normalized_df\n",
    "\n",
    "        mydb.close()\n",
    "\n",
    "initiate_model_instance_bi = bi_initiate_model()\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "SEQ_LEN = 180\n",
    "SEQ_STEP = 60\n",
    "PRED_SIZE = 4\n",
    "D_MODEL = 16\n",
    "NUM_HEADS = 4\n",
    "D_FF = 2048 \n",
    "\n",
    "# neural network functions\n",
    "def linear_activation(input, weights, biases):\n",
    "    batch_size, seq_length, d_model = input.shape  # extract input shape\n",
    "    \n",
    "    x_flat = np.reshape(input, (batch_size * seq_length, d_model))  # flatten input into (batch_size, d_model)\n",
    "    z_flat = np.dot(x_flat, weights.T) + biases\n",
    "    \n",
    "    return np.reshape(z_flat, (batch_size, seq_length, -1))  # reshape back to (batch_size, seq_length, d_model)\n",
    "\n",
    "\n",
    "def relu(input):\n",
    "    batch_size, seq_length, d_model = input.shape  # extract input shape\n",
    "    \n",
    "    x_flat = np.reshape(input, (batch_size * seq_length, d_model))  # flatten input into (batch_size, d_model)\n",
    "    a_flat = np.maximum(x_flat, 0) \n",
    "    \n",
    "    return np.reshape(a_flat, (batch_size, seq_length, -1))  # reshape back to (batch_size, seq_length, d_model)\n",
    "\n",
    "\n",
    "def sigmoid(input):\n",
    "    batch_size, seq_length, d_model = input.shape  # extract input shape\n",
    "    \n",
    "    x_flat = np.reshape(input, (batch_size * seq_length, d_model))  # flatten input into (batch_size, d_model)\n",
    "    a_flat = 1 / (1 + np.exp(-x_flat))\n",
    "    \n",
    "    return np.reshape(a_flat, (batch_size, seq_length, -1))  # reshape back to (batch_size, seq_length, d_model)\n",
    "\n",
    "\n",
    "def softmax(input):\n",
    "    batch_size, seq_length, d_model = input.shape\n",
    "    \n",
    "    x_flat = np.reshape(input, (batch_size * seq_length, d_model)).T  # flatten input into (batch_size, d_model)\n",
    "    a_flat = np.exp(x_flat) / (np.sum(np.exp(x_flat), axis=0) + 1e-8)\n",
    "    \n",
    "    return np.reshape(a_flat.T, (batch_size, seq_length, -1))  # reshape back to (batch_size, seq_length, d_model)\n",
    "\n",
    "\n",
    "def layer_normalization(input, gamma, beta):\n",
    "    mean = np.mean(input, axis=-1, keepdims=True)  # get mean in each axis\n",
    "    std = np.std(input, axis=-1, keepdims=True)  # get standard deviation in each axis\n",
    "    \n",
    "    normalized = (input - mean) / (std + 1e-8)  # normalized activations \n",
    "    \n",
    "    # reshape parameters to fit the input shape\n",
    "    gamma = np.reshape(gamma, (1, 1, -1))\n",
    "    beta = np.reshape(beta, (1, 1, -1))\n",
    "    \n",
    "    return gamma * normalized + beta  # normalized activations with size of (batch_size, seq_length, d_model)\n",
    "\n",
    "# positional encoding\n",
    "def positional_encoding(input, n=10000):\n",
    "    batch_size, seq_length, d_model = input.shape\n",
    "    \n",
    "    pe = np.zeros(shape=(seq_length, d_model))\n",
    "    for k in range(seq_length):\n",
    "        for i in np.arange(int(d_model / 2)):\n",
    "            denominator = np.power(n, 2 * i / d_model)\n",
    "            pe[k, 2*i] = np.sin(k / denominator)\n",
    "            pe[k, 2*i+1] = np.cos(k / denominator)\n",
    "            \n",
    "    return input + pe  # add positional encoding to input\n",
    "\n",
    "\n",
    "# multi-head attention\n",
    "def split_heads(input, num_heads):\n",
    "    batch_size, seq_length, d_model = input.shape\n",
    "    \n",
    "    assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "    head_dim = d_model // num_heads\n",
    "    \n",
    "    heads = np.reshape(input, (batch_size, seq_length, num_heads, head_dim))\n",
    "    heads = np.transpose(heads, (0, 2, 1, 3))\n",
    "    \n",
    "    return heads  # attention heads with size of (batch_size, num_heads, seq_length, head_dim)\n",
    "\n",
    "\n",
    "def combine_heads(input):\n",
    "    combined = np.transpose(input, (0, 2, 1, 3))\n",
    "    combined = np.reshape(combined, (combined.shape[0], combined.shape[1], -1))\n",
    "    \n",
    "    return combined  # combined attention heads with size of (batch_size, seq_length, d_model)\n",
    "\n",
    "\n",
    "def scaled_dot_product_attention(query, key, value):\n",
    "    batch_size, num_heads, seq_length, head_dim = query.shape\n",
    "    \n",
    "    # convert input into (batch_size, seq_length, d_model)\n",
    "    query = np.reshape(query, (batch_size * num_heads, seq_length, head_dim))  \n",
    "    key = np.reshape(key, (batch_size * num_heads, seq_length, head_dim))\n",
    "    value = np.reshape(value, (batch_size * num_heads, seq_length, head_dim))\n",
    "    \n",
    "    key = np.transpose(key, (0, 2, 1))  # transpose key\n",
    "    attn_scores = np.matmul(query, key) / math.sqrt(head_dim)  # get dot product attention\n",
    "    \n",
    "    attn_scores = softmax(attn_scores)  # convert attention scores into probabilities\n",
    "    \n",
    "    value = np.matmul(attn_scores, value)  # embed attention scores into value\n",
    "    \n",
    "    return np.reshape(attn_scores, (batch_size, num_heads, seq_length, seq_length)), np.reshape(value, (batch_size, num_heads, seq_length, head_dim))  # reshape to original size\n",
    "\n",
    "\n",
    "def multi_head_self_attention(query, key, value, num_heads, params):\n",
    "    query = split_heads(linear_activation(query, params[0], params[1]), num_heads)\n",
    "    key = split_heads(linear_activation(key, params[2], params[3]), num_heads)\n",
    "    value = split_heads(linear_activation(value, params[4], params[5]), num_heads)\n",
    "    \n",
    "    attn_scores, attn_output = scaled_dot_product_attention(query, key, value)\n",
    "    attn_output = linear_activation(combine_heads(attn_output), params[6], params[7])\n",
    "    \n",
    "    return attn_scores, attn_output\n",
    "\n",
    "\n",
    "# feed forward network\n",
    "def feed_forward_network(input, params):\n",
    "    out = linear_activation(input, params[0], params[1])\n",
    "    out = relu(out)\n",
    "    out = linear_activation(out, params[2], params[3])\n",
    "    \n",
    "    return out\n",
    "\n",
    "# decoder layer\n",
    "def transformer_encoder(input, num_heads, params):\n",
    "    attn_scores, attn_out = multi_head_self_attention(\n",
    "        query=input,\n",
    "        key=input, \n",
    "        value=input, \n",
    "        num_heads=num_heads, \n",
    "        params=params[:8])\n",
    "    norm1 = layer_normalization(input + attn_out, params[12], params[13])\n",
    "    ff_out = feed_forward_network(norm1, params[8:12])\n",
    "    norm2 = layer_normalization(norm1 + ff_out, params[14], params[15])\n",
    "    \n",
    "    return attn_scores, norm2\n",
    "\n",
    "\n",
    "# model\n",
    "def transformer(input, num_heads, params):\n",
    "    out = positional_encoding(input)\n",
    "    \n",
    "    # decoder layers\n",
    "    _, out = transformer_encoder(out, num_heads, params[:16])\n",
    "    scores, out = transformer_encoder(out, num_heads, params[16:32])\n",
    "    \n",
    "    # final layer\n",
    "    out = linear_activation(out, params[32], params[33])\n",
    "    out = sigmoid(out)\n",
    "    \n",
    "    return scores, out\n",
    "\n",
    "# load parameters from file\n",
    "with open(\"../bidirectional_parameters.json\", \"r\") as parameters:\n",
    "    saved_params = json.load(parameters)\n",
    "\n",
    "# iterate through layer parameters\n",
    "params = []\n",
    "for key in saved_params.keys():\n",
    "    param = np.asarray(saved_params[key], dtype=np.float32)  # convert saved parameters back to numpy\n",
    "    params.append(param)\n",
    "    \n",
    "len(params)  # print number of layer parameters\n",
    "\n",
    "def inverse_transform(data):\n",
    "    data_min = initiate_model_instance_bi.dataset_min[['Waterlevel', 'Waterlevel.1', 'Waterlevel.2', 'Waterlevel.3']].to_numpy()\n",
    "    data_max = initiate_model_instance_bi.dataset_max[['Waterlevel', 'Waterlevel.1', 'Waterlevel.2', 'Waterlevel.3']].to_numpy()\n",
    "    \n",
    "    return (data_max - data_min) * data + data_min\n",
    "\n",
    "\n",
    "def mean_absolute_error(y_true, y_pred):\n",
    "    return np.mean(np.abs(y_true - y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = initiate_model_instance_bi.normalized_df['2012-01-01':].values\n",
    "dataset_len = len(test_data) - (SEQ_LEN + SEQ_STEP) + 1\n",
    "\n",
    "# prepare batches\n",
    "batches = []\n",
    "for index in range(dataset_len):\n",
    "    in_start = index\n",
    "    in_end = in_start + SEQ_LEN\n",
    "    out_start = index + SEQ_STEP\n",
    "    out_end = out_start + SEQ_LEN\n",
    "    \n",
    "    input = test_data[in_start:in_end]\n",
    "    label = test_data[out_start:out_end, :PRED_SIZE]\n",
    "    \n",
    "    batches.append((np.array(input), np.array(label)))\n",
    "\n",
    "# measure accuracy of each window\n",
    "accuracy = []\n",
    "predictions = []\n",
    "for input, label in batches:\n",
    "    \n",
    "    input = np.reshape(input, (1, SEQ_LEN, D_MODEL))\n",
    "    scores, pred = transformer(input=input, num_heads=NUM_HEADS, params=params)  # make forecast\n",
    "    pred = np.reshape(pred, (SEQ_LEN, PRED_SIZE))  \n",
    "    pred = inverse_transform(pred[:, :4])  # scale output to original value\n",
    "    pred = pred[-SEQ_STEP:]   # get only the forecast window\n",
    "    \n",
    "    ground = inverse_transform(label[:, :4])  # scale output to original value\n",
    "    ground = ground[-SEQ_STEP:]  # get only the forecast window\n",
    "    \n",
    "    accuracy.append(mean_absolute_error(ground, pred))  # collect mean absolute error of each window\n",
    "    predictions.append(np.concatenate((pred[0], ground[0])))  # collect first element of output\n",
    "    \n",
    "accuracy_df = pd.DataFrame(np.array(accuracy), columns=['MAE'])\n",
    "predictions_df = pd.DataFrame(np.array(predictions), columns=['P.Waterlevel', 'P.Waterlevel-1', 'P.Waterlevel-2', 'P.Waterlevel-3', 'T.Waterlevel', 'T.Waterlevel-1', 'T.Waterlevel-2', 'T.Waterlevel-3'])\n",
    "metric_df = pd.concat([accuracy_df, predictions_df], axis=1)\n",
    "metric_df.index = initiate_model_instance_bi.rawData.index[-len(metric_df):]\n",
    "\n",
    "metric_df.to_csv('bidirectional_date_range.csv')  # save test results\n",
    "\n",
    "pass_metric_df = pd.read_csv('bidirectional_date_range.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "bidf = pd.read_csv('bidirectional_date_range.csv')\n",
    "\n",
    "rcdf = pd.read_csv('numpy_rivercast_date_range.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Mean Absolute Error: 0.3468102676265324\n",
      "Mean Absolute Error STD: 0.2381076048785913\n"
     ]
    }
   ],
   "source": [
    "print('Average Mean Absolute Error:', rcdf['MAE'].mean())\n",
    "print('Mean Absolute Error STD:', rcdf['MAE'].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, jsonify, send_file, g\n",
    "import io\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')  # Use a non-GUI backend\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import csv\n",
    "from datetime import datetime, timedelta\n",
    "from datetime import date\n",
    "import pymysql\n",
    "import mysql.connector\n",
    "from sqlalchemy import create_engine, inspect, DateTime\n",
    "from PIL import Image\n",
    "from flask_cors import CORS  # Import CORS from flask_cors\n",
    "import pandas as pd\n",
    "from sqlalchemy.exc import NoSuchTableError\n",
    "from io import BytesIO\n",
    "import os\n",
    "\n",
    "\n",
    "app = Flask(__name__)\n",
    "CORS(app, supports_credentials=True)\n",
    "\n",
    "\n",
    "DB_CONFIG = {\n",
    "    \"host\": \"localhost\",\n",
    "    \"user\": \"root\",\n",
    "    \"password\": \"pmcm4\",\n",
    "    \"database\": \"rivercast_model\"\n",
    "}\n",
    "\n",
    "def get_db():\n",
    "    if 'db' not in g:\n",
    "        g.db = mysql.connector.connect(**DB_CONFIG)\n",
    "    return g.db\n",
    "\n",
    "def close_db(e=None):\n",
    "    db = g.pop('db', None)\n",
    "    if db is not None:\n",
    "        db.close()\n",
    "\n",
    "@app.before_request\n",
    "def before_request():\n",
    "    g.db = get_db()\n",
    "\n",
    "@app.teardown_request\n",
    "def teardown_request(e=None):\n",
    "    close_db()\n",
    "\n",
    "engine = create_engine(\"mysql+pymysql://\" + \"root\" + \":\" + \"pmcm4\" + \"@\" + \"localhost\" + \"/\" + \"rivercast_model\")\n",
    "\n",
    "# RIVERCAST APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17042"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df22 = pd.read_csv('numpy_rivercast_date_range.csv')\n",
    "\n",
    "df22.set_index('Datetime', inplace=True)\n",
    "df22.to_sql(name='rivercast_daterange_data', con=engine, index=True, index_label='Datetime', if_exists='replace', method='multi', dtype={'Datetime': DateTime(50)})\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
